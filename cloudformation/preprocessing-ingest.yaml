# A template that creates the ingest system for sensor data
#
# Copyright 2018 Melon Software Ltd (UK), all rights reserved
#
AWSTemplateFormatVersion: "2010-09-09"
Description: "Creates ingest pipeline for preprocessing infrastructure"

Parameters:

    Environment:
        Type: "String"
        Description: "The name of the Environment"

    S3BucketName:
        Type: "String"
        Description: "The name of the S3 ingest bucket"

    StateMachineArn:
        Type: "String"
        Description: "The ARN of the State Machine"

Metadata:
    "AWS::CloudFormation::Interface":
        ParameterLabels:
            Environment: { default: "Environment" }
            StateMachineArn: { default: "State Machine ARN" }

Resources:

    ##########################################################################################################
    ##  IAM
    ##########################################################################################################

    LambdaTriggerRole:
        Type: "AWS::IAM::Role"
        Properties:
            AssumeRolePolicyDocument:
                Version: "2012-10-17"
                Statement:
                  - Effect: "Allow"
                    Principal: { Service: [ "lambda.amazonaws.com" ] }
                    Action: "sts:AssumeRole"
            ManagedPolicyArns:
              - "arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"
            Policies:
              - PolicyName: "default"
                PolicyDocument:
                    Version: "2012-10-17"
                    Statement:
                      - Action:
                          - "logs:CreateLogGroup"
                          - "logs:CreateLogStream"
                          - "logs:PutLogEvents"
                        Effect: "Allow"
                        Resource: "*"

                      - Action:
                          - "s3:DeleteObject"
                          - "s3:GetObject"
                          - "s3:ListBucket"
                          - "s3:PutObject"
                          - "states:StartExecution"
                          - "cloudwatch:PutMetricData"
                        Effect: "Allow"
                        Resource: "*"

                      - Action:
                          - "dynamodb:PutItem"
                          - "dynamodb:Query"
                        Effect: "Allow"
                        Resource: { "Fn::Sub": "arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/preprocessing-${Environment}-ingest-*" }
            RoleName: { "Fn::Sub": "preprocessing-${Environment}-ingest-lambda-${AWS::Region}" }

    ##########################################################################################################
    ##  DATABASE
    ##########################################################################################################

    UploadedPacketsTable:
        Type: "AWS::DynamoDB::Table"
        Properties:
            TableName: { "Fn::Sub": "preprocessing-${Environment}-ingest-packets" }
            AttributeDefinitions:
              - { AttributeName: "sensorDataFilename", AttributeType: "S" }
              - { AttributeName: "partUploadTime", AttributeType: "S" }
            KeySchema:
              - { AttributeName: "sensorDataFilename", KeyType: "HASH" }
              - { AttributeName: "partUploadTime", KeyType: "RANGE" }
            ProvisionedThroughput:
                ReadCapacityUnits: 1
                WriteCapacityUnits: 1

    ##########################################################################################################
    ##  LAMBDA
    ##########################################################################################################

    LambdaTrigger:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: |
                    import boto3, json, os, time, datetime
                    dynamodb = boto3.resource('dynamodb').Table(os.environ['DYNAMODB_TABLE_NAME'])

                    def push_to_dynamodb(s3_bucket, s3_key, base_filename, upload_time, version):
                        dynamodb.put_item(Item={
                            'sensorDataFilename': base_filename,
                            'partUploadTime': upload_time,
                            's3Bucket': s3_bucket,
                            's3Key': s3_key,
                            'version': version,
                        })

                    def handler(event, context):
                        print(event)
                        record = event['Records'][0]
                        s3_bucket = record['s3']['bucket']['name']
                        s3_key = record['s3']['object']['key'].split('/')[-1]

                        s3_object = boto3.resource('s3').Object(s3_bucket, record['s3']['object']['key'])
                        upload_time = s3_object.last_modified.isoformat().rsplit('+', 1)[0]

                        if '_' in s3_key:
                            # A multipart upload
                            s3_basepath, part = s3_key.split('_')
                            if part == 'v1.0':
                                push_to_dynamodb(s3_bucket, s3_key, s3_basepath, upload_time, '1.0')
                                trigger_sfn(s3_bucket, s3_basepath, '1.0')
                            else:
                                # Assume version 2
                                push_to_dynamodb(s3_bucket, s3_key, s3_basepath, upload_time, '2.3')
                                if part == 'complete':
                                    trigger_sfn(s3_bucket, s3_basepath, '2.3')

                        else:
                            # An un-suffixed file.  We don't do these any more (must be a bug)
                            return

                    def trigger_sfn(s3_bucket, s3_basepath, version):
                        execution_name = '{}-{}'.format(s3_basepath, int(time.time()))
                        sfn_client = boto3.client('stepfunctions')
                        res = sfn_client.start_execution(
                            stateMachineArn=os.environ['STATE_MACHINE_ARN'],
                            name=execution_name,
                            input=json.dumps({
                                "Meta": {
                                    "ExecutionArn": "{}:{}".format(os.environ['STATE_MACHINE_ARN'], execution_name),
                                    "ExecutionName": execution_name,
                                },
                                "SourceEvent": {
                                    "SensorDataFilename": s3_basepath,
                                    "SensorDataFileVersion": version,
                                }
                            })
                        )


            Environment:
                Variables:
                    DYNAMODB_TABLE_NAME: { Ref: "UploadedPacketsTable" }
                    ENVIRONMENT: { Ref: "Environment" }
                    STATE_MACHINE_ARN: { Ref: "StateMachineArn" }
            Handler: "index.handler"
            MemorySize: "256"
            Runtime: "python3.6"
            Timeout: "60"
            Role: { "Fn::GetAtt" : [ "LambdaTriggerRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-ingest-trigger" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-ingest-trigger" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }
              - { Key: "Service", Value: "ingest" }

    LambdaTriggerPermission:
        Type: "AWS::Lambda::Permission"
        Properties:
            Action: "lambda:InvokeFunction"
            FunctionName: { "Fn::GetAtt": [ "LambdaTrigger", "Arn" ] }
            Principal: "s3.amazonaws.com"
            SourceArn: { "Fn::Sub": "arn:aws:s3:::${S3BucketName}" }
            SourceAccount: { Ref : "AWS::AccountId" }

Outputs:
    TriggerLambdaArn:
        Description: "The ARN of the trigger lambda"
        Value: { "Fn::GetAtt": [ "LambdaTrigger", "Arn" ] }

# A template that creates the ingest system for sensor data
#
# Copyright 2018 Melon Software Ltd (UK), all rights reserved
#
AWSTemplateFormatVersion: "2010-09-09"
Description: "Creates ingest pipeline for preprocessing infrastructure"

Parameters:

    Environment:
        Type: "String"
        Description: "The name of the Environment"

    S3BucketName:
        Type: "String"
        Description: "The name of the S3 ingest bucket"

    StateMachineArn:
        Type: "String"
        Description: "The ARN of the State Machine"

Metadata:
    "AWS::CloudFormation::Interface":
        ParameterLabels:
            Environment: { default: "Environment" }
            StateMachineArn: { default: "State Machine ARN" }

Resources:

    ##########################################################################################################
    ##  IAM
    ##########################################################################################################

    LambdaTriggerRole:
        Type: "AWS::IAM::Role"
        Properties:
            AssumeRolePolicyDocument:
                Version: "2012-10-17"
                Statement:
                  - Effect: "Allow"
                    Principal: { Service: [ "lambda.amazonaws.com" ] }
                    Action: "sts:AssumeRole"
            ManagedPolicyArns:
              - "arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"
              - "arn:aws:iam::aws:policy/service-role/AWSLambdaDynamoDBExecutionRole"
              - "arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess"
              - { "Fn::Sub": "arn:aws:iam::${AWS::AccountId}:policy/infrastructure-${Environment}-querypostgres" }
              - { Ref: "SessionEventsReadPolicy" }
              - { Ref: "SessionEventsWritePolicy" }
            Policies:
              - PolicyName: "default"
                PolicyDocument:
                    Version: "2012-10-17"
                    Statement:
                      - Action:
                          - "cloudwatch:PutMetricData"
                          - "s3:ListBucket"
                          - "s3:GetObject"
                        Effect: "Allow"
                        Resource: "*"

                      - Action:
                          - "states:StartExecution"
                        Effect: "Allow"
                        Resource: { Ref: "StateMachineArn" }
            RoleName: { "Fn::Sub": "preprocessing-${Environment}-ingest-lambda-${AWS::Region}" }

    ##########################################################################################################
    ##  DATABASE
    ##########################################################################################################

    SessionEventsTable:
        Type: "AWS::DynamoDB::Table"
        Properties:
            TableName: { "Fn::Sub": "preprocessing-${Environment}-ingest-sessions" }
            AttributeDefinitions:
              - { AttributeName: "id", AttributeType: "S" }
              - { AttributeName: "userId", AttributeType: "S" }
              - { AttributeName: "teamId", AttributeType: "S" }
              - { AttributeName: "trainingGroupId", AttributeType: "S" }
              - { AttributeName: "eventDate", AttributeType: "S" }
            KeySchema:
              - { AttributeName: "id", KeyType: "HASH" }
            ProvisionedThroughput:
                ReadCapacityUnits: 1
                WriteCapacityUnits: 1
            GlobalSecondaryIndexes:
              - IndexName: "userId-eventDate"
                KeySchema:
                  - { AttributeName: "userId", KeyType: "HASH" }
                  - { AttributeName: "eventDate", KeyType: "RANGE" }
                Projection:
                    ProjectionType: "ALL"
                ProvisionedThroughput:
                    ReadCapacityUnits: 1
                    WriteCapacityUnits: 1
              - IndexName: "teamId-eventDate"
                KeySchema:
                  - { AttributeName: "teamId", KeyType: "HASH" }
                  - { AttributeName: "eventDate", KeyType: "RANGE" }
                Projection:
                    ProjectionType: "ALL"
                ProvisionedThroughput:
                    ReadCapacityUnits: 1
                    WriteCapacityUnits: 1
              - IndexName: "trainingGroupId-eventDate"
                KeySchema:
                  - { AttributeName: "trainingGroupId", KeyType: "HASH" }
                  - { AttributeName: "eventDate", KeyType: "RANGE" }
                Projection:
                    ProjectionType: "ALL"
                ProvisionedThroughput:
                    ReadCapacityUnits: 1
                    WriteCapacityUnits: 1
            StreamSpecification:
                StreamViewType: "NEW_AND_OLD_IMAGES"

    SessionEventsReadPolicy:
        Type: "AWS::IAM::ManagedPolicy"
        Properties:
            Description: "Allows entities to read from the preprocessing sessions table"
            ManagedPolicyName: { "Fn::Sub": "preprocessing-${Environment}-ingest-sessions-read" }
            Path: "/"
            PolicyDocument:
                Version: "2012-10-17"
                Statement:
                  - Action:
                      - "dynamodb:GetItem"
                      - "dynamodb:Query"
                    Effect: "Allow"
                    Resource: { "Fn::GetAtt": [ "SessionEventsTable", "Arn" ] }

    SessionEventsWritePolicy:
        Type: "AWS::IAM::ManagedPolicy"
        Properties:
            Description: "Allows entities to write to the preprocessing sessions table"
            ManagedPolicyName: { "Fn::Sub": "preprocessing-${Environment}-ingest-sessions-write" }
            Path: "/"
            PolicyDocument:
                Version: "2012-10-17"
                Statement:
                  - Action:
                      - "dynamodb:UpdateItem"
                      - "dynamodb:DeleteItem"
                    Effect: "Allow"
                    Resource: { "Fn::GetAtt": [ "SessionEventsTable", "Arn" ] }

    ##########################################################################################################
    ##  LAMBDA
    ##########################################################################################################

    LambdaTrigger:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: |
                    import boto3, os, datetime
                    from boto3.dynamodb.conditions import Attr
                    ddb_parts_table = boto3.resource('dynamodb').Table(os.environ['DYNAMODB_PARTS_TABLE_NAME'])
                    ddb_session_events_table = boto3.resource('dynamodb').Table(os.environ['DYNAMODB_SESSION_EVENTS_TABLE_NAME'])

                    def handler(event, _):
                        print(event)
                        record = event['Records'][0]
                        s3_bucket = record['s3']['bucket']['name']
                        s3_key = record['s3']['object']['key'].split('/')[-1]

                        s3_object = boto3.resource('s3').Object(s3_bucket, record['s3']['object']['key'])
                        upload_time = s3_object.last_modified.strftime("%Y-%m-%dT%H:%M:%SZ")
                        now_time = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")

                        session_event_updates = DynamodbUpdate()

                        if '_' in s3_key:
                            # A multipart upload
                            s3_basepath, part = s3_key.split('_')
                            if part == 'v1.0':
                                version = '1.0'
                            else:
                                # Assume version 2
                                version = '2.3'

                            session_event_updates.add("s3Files", {s3_key})
                            session_event_updates.set("updatedDate", now_time)
                            session_event_updates.set("version", version)

                            if part == 'complete' or version == '1.0':
                                # Ready for processing
                                session_event_updates.set("sessionStatus", 'UPLOAD_COMPLETE')

                            ddb_session_events_table.update_item(
                                Key={'id': s3_basepath},
                                ConditionExpression=Attr('id').not_exists() | Attr('sessionStatus').eq('UPLOAD_IN_PROGRESS'),
                                UpdateExpression=session_event_updates.update_expression,
                                ExpressionAttributeValues=session_event_updates.parameters,
                            )
                            # TODO deal with the exception when the conditional check fails (will prevent reingesting files)

                        else:
                            # An un-suffixed file.  We don't do these any more (must be a bug)
                            return

                    class DynamodbUpdate:
                        def __init__(self):
                            self._add = []
                            self._set = []
                            self._parameters = {}

                        def set(self, field, value):
                            self._set.append("{field} = :{field}".format(field=field))
                            self._parameters[':' + field] = value

                        def add(self, field, value):
                            self._add.append("{field} :{field}".format(field=field))
                            self._parameters[':' + field] = value

                        @property
                        def update_expression(self):
                            return 'SET {} ADD {}'.format(', '.join(self._set), ', '.join(self._add))

                        @property
                        def parameters(self):
                            return self._parameters
            Environment:
                Variables:
                    DYNAMODB_SESSION_EVENTS_TABLE_NAME: { Ref: "SessionEventsTable" }
                    ENVIRONMENT: { Ref: "Environment" }
                    STATE_MACHINE_ARN: { Ref: "StateMachineArn" }
            Handler: "index.handler"
            MemorySize: "256"
            Runtime: "python3.6"
            Timeout: "60"
            Role: { "Fn::GetAtt" : [ "LambdaTriggerRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-ingest-trigger" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-ingest-trigger" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }
              - { Key: "Service", Value: "ingest" }

    SessionsDynamodbTriggerLambda:
        Type: "AWS::Lambda::Function"
        Properties:
            Code:
                ZipFile: |
                    import boto3, json, os, time
                    from boto3.dynamodb.conditions import Attr

                    def handler(event, _):
                        print(event)
                        for record in event['Records']:
                            new_object = extract_dynamodb_object(record['dynamodb'].get('NewImage', {}))
                            old_object = extract_dynamodb_object(record['dynamodb'].get('OldImage', {}))

                            if record['eventName'] == 'MODIFY':
                                new_status = new_object.get('sessionStatus', '???')
                                old_status = old_object.get('sessionStatus', '???')

                                if new_status == 'UPLOAD_COMPLETE' and old_status != 'UPLOAD_COMPLETE':
                                    # Begin processing
                                    if 'version' in new_object:
                                        trigger_sfn(new_object['id'], new_object['version'])

                            elif record['eventName'] == 'INSERT':
                                if 'eventDate' not in new_object or not new_object['eventDate']:
                                    # Need to pull in the data from postgres
                                    query_results = query_postgres(
                                        """SELECT
                                          happened_at::timestamp(0) AS event_date,
                                          created_at::timestamp(0) AS created_date,
                                          session_events.user_id AS user_id,
                                          team_id AS team_id,
                                          training_group_ids[1] AS tg_id
                                        FROM session_events
                                        LEFT JOIN teams_users ON session_events.user_id = teams_users.user_id
                                        WHERE id = %s""",
                                        [new_object['id']]
                                    )
                                    print(query_results)
                                    if len(query_results):
                                        boto3.resource('dynamodb').Table(os.environ['DYNAMODB_TABLE_NAME']).update_item(
                                            Key={'id': new_object['id']},
                                            ConditionExpression=Attr('id').not_exists() | Attr('sessionStatus').eq('UPLOAD_IN_PROGRESS'),
                                            UpdateExpression="""SET
                                                eventDate = :event_date,
                                                createdDate = :created_date,
                                                userId = :user_id,
                                                teamId = :team_id,
                                                trainingGroupId = :tg_id""",
                                            ExpressionAttributeValues={':' + k: v for k, v in query_results[0].items()},
                                        )

                    def trigger_sfn(session_event_id, version):
                        execution_name = '{}-{}'.format(session_event_id, int(time.time()))
                        sfn_client = boto3.client('stepfunctions')
                        sfn_client.start_execution(
                            stateMachineArn=os.environ['STATE_MACHINE_ARN'],
                            name=execution_name,
                            input=json.dumps({
                                "Meta": {
                                    "ExecutionArn": "{}:{}".format(os.environ['STATE_MACHINE_ARN'], execution_name),
                                    "ExecutionName": execution_name,
                                },
                                "SourceEvent": {
                                    "SensorDataFilename": session_event_id,
                                    "SensorDataFileVersion": version,
                                }
                            })
                        )

                    def query_postgres(query, parameters):
                        res = json.loads(boto3.client('lambda').invoke(
                            FunctionName='arn:aws:lambda:us-west-2:887689817172:function:infrastructure-{}-querypostgres'.format(os.environ['ENVIRONMENT']),
                            Payload=json.dumps({
                                "Queries": [{"Query": query, "Parameters": parameters}],
                                "Config": {"ENVIRONMENT": os.environ['ENVIRONMENT']}
                            }),
                        )['Payload'].read().decode('utf-8'))
                        result, error = res['Results'][0], res['Errors'][0]
                        if error is not None:
                            raise Exception(error)
                        else:
                            return result if len(result) else []

                    def extract_dynamodb_object(record):
                        def extract_dynamodb_row(value):
                            if 'S' in value:
                                return value['S']
                            elif 'N' in value:
                                return float(value['N'])
                            elif 'SS' in value:
                                return list(value['SS'])
                            else:
                                raise Exception()
                        return {k: extract_dynamodb_row(v) for k, v in record.items()}

            Environment:
                Variables:
                    DYNAMODB_TABLE_NAME: { Ref: "SessionEventsTable" }
                    ENVIRONMENT: { Ref: "Environment" }
                    STATE_MACHINE_ARN: { Ref: "StateMachineArn" }
            Handler: "index.handler"
            MemorySize: "256"
            Runtime: "python3.6"
            Timeout: "300"
            Role: { "Fn::GetAtt" : [ "LambdaTriggerRole", "Arn" ] }
            FunctionName: { "Fn::Sub": "preprocessing-${Environment}-ingest-sessions-stream" }
            Tags:
              - { Key: "Name", Value: { "Fn::Sub": "preprocessing-${Environment}-ingest-sessions-stream" } }
              - { Key: "Management", Value: "managed" }
              - { Key: "Project", Value: "preprocessing" }
              - { Key: "Environment", Value: { Ref: "Environment" } }
              - { Key: "Service", Value: "ingest" }

    SessionsDynamodbTriggerMapping:
        Type: "AWS::Lambda::EventSourceMapping"
        Properties:
            BatchSize: 1
            Enabled: true
            EventSourceArn: { "Fn::GetAtt": [ "SessionEventsTable", "StreamArn" ] }
            FunctionName: { Ref: "SessionsDynamodbTriggerLambda" }
            StartingPosition: "LATEST"

    LambdaTriggerPermission:
        Type: "AWS::Lambda::Permission"
        Properties:
            Action: "lambda:InvokeFunction"
            FunctionName: { "Fn::GetAtt": [ "LambdaTrigger", "Arn" ] }
            Principal: "s3.amazonaws.com"
            SourceArn: { "Fn::Sub": "arn:aws:s3:::${S3BucketName}" }
            SourceAccount: { Ref : "AWS::AccountId" }

Outputs:
    TriggerLambdaArn:
        Description: "The ARN of the trigger lambda"
        Value: { "Fn::GetAtt": [ "LambdaTrigger", "Arn" ] }
    SessionEventsTableName:
        Description: "The name of the session events table"
        Value: { Ref: "SessionEventsTable" }
